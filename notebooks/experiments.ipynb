{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80V_CBzzLMU1"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from gym import Space, spaces\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from prisoners_dilemma import utils\n",
    "from prisoners_dilemma.env import PrisonersDilemmaEnv\n",
    "\n",
    "sns.set('notebook', font_scale=1.1, rc={'figure.figsize': (7, 4)})\n",
    "sns.set_style('ticks', rc={'figure.facecolor': 'none', 'axes.facecolor': 'none'})\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments [standard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup config\n",
    "config = {\n",
    "    'num_simuls': 1,\n",
    "    'num_episodes': 50,\n",
    "    'num_agents': 2,\n",
    "    'num_actions': 2,\n",
    "    'verbose': False,\n",
    "    'init_type':'zeros',\n",
    "}\n",
    "\n",
    "config['payoffs'] = {\n",
    "    'reward_payoff': 2,\n",
    "    'tempta_payoff': 3,\n",
    "    'sucker_payoff': 0,\n",
    "    'punish_payoff': 1,\n",
    "}\n",
    "\n",
    "config['params'] = {\n",
    "    'alpha': np.array([0.1, 0.1]),\n",
    "    'eps': np.array([0.35, 0.35]),\n",
    "    'gamma': np.array([0.5, 0.5]),\n",
    "}\n",
    "\n",
    "# Run experiments\n",
    "q_traj_one, q_traj_two, rewards_seq, action_seq = utils.run_standard_ipd_exp(config)\n",
    "\n",
    "# Visualize trajectories and actions\n",
    "utils.make_q_vals_fig_standard(\n",
    "    action_seq=action_seq,\n",
    "    config=config,\n",
    "    q_traj_one=q_traj_one,\n",
    "    q_traj_two=q_traj_two,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments [with observations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup config\n",
    "config = {\n",
    "    'num_simuls': 10,\n",
    "    'num_episodes': 1000,\n",
    "    'num_agents': 2,\n",
    "    'num_actions': 2,\n",
    "    'verbose': False,\n",
    "    'init_type':'zeros',\n",
    "}\n",
    "\n",
    "config['payoffs'] = {\n",
    "    'reward_payoff': 2,\n",
    "    'tempta_payoff': 3,\n",
    "    'sucker_payoff': 0,\n",
    "    'punish_payoff': 1,\n",
    "}\n",
    "\n",
    "config['params'] = {\n",
    "    'alpha': np.array([0.1, 0.1]),\n",
    "    'eps': np.array([0.1, 0.1]),\n",
    "    'gamma': np.array([0.5, 0.5]),\n",
    "}\n",
    "\n",
    "# Create arrays\n",
    "q_traj_one = np.zeros((config[\"num_simuls\"], config[\"num_episodes\"], config[\"num_actions\"]))\n",
    "q_traj_two = np.zeros((config[\"num_simuls\"], config[\"num_episodes\"], config[\"num_actions\"], config[\"num_actions\"]))\n",
    "rewards_seq = np.zeros((config[\"num_simuls\"], config[\"num_episodes\"], config[\"num_agents\"]))\n",
    "action_seq = np.zeros((config[\"num_simuls\"], config[\"num_episodes\"], config[\"num_agents\"]), dtype=int)\n",
    "\n",
    "for simul_i in range(config['num_simuls']):\n",
    "    q_traj_one[simul_i, :, :], q_traj_two[simul_i, :, :], rewards_seq[simul_i, :, :], action_seq[simul_i, :, :] = utils.run_extended_ipd_exp(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_play_one = pd.DataFrame({\n",
    "    'Simulation' : np.repeat(np.arange(config['num_simuls']), config['num_episodes']),\n",
    "    'Episode'    : np.tile(np.arange(config['num_episodes']), config['num_simuls']),\n",
    "    'Q_D'        : q_traj_one[:, :, 0].flatten(),\n",
    "    'Q_C'        : q_traj_one[:, :, 1].flatten(),\n",
    "    'Rewards'    : rewards_seq[:, :, 0].flatten()\n",
    "})\n",
    "\n",
    "df_play_two = pd.DataFrame({\n",
    "    'Simulation' : np.repeat(np.arange(config['num_simuls']), config['num_episodes']),\n",
    "    'Episode'    : np.tile(np.arange(config['num_episodes']), config['num_simuls']),\n",
    "    'Q_D_cond_D' : q_traj_two[:, :, 0, 0].flatten(),\n",
    "    'Q_D_cond_C' : q_traj_two[:, :, 0, 1].flatten(),\n",
    "    'Q_C_cond_D' : q_traj_two[:, :, 1, 0].flatten(),\n",
    "    'Q_C_cond_C' : q_traj_two[:, :, 1, 1].flatten(),\n",
    "    'Rewards'    : rewards_seq[:, :, 1].flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set('notebook', font_scale=1.1, rc={'figure.figsize': (10, 5)})\n",
    "sns.set_style('ticks', rc={'figure.facecolor': 'none', 'axes.facecolor': 'none'})\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharey=True, sharex=True)\n",
    "fig.suptitle(f'Q-values over s={config[\"num_simuls\"]} simulations', y=1.05)\n",
    "\n",
    "sns.lineplot(data=df_play_one, x='Episode', y='Q_D', errorbar='sd', label=r'$Q^{1}_{D}$', ax=axs[0])\n",
    "sns.lineplot(data=df_play_one, x='Episode', y='Q_C', errorbar='sd', label=r'$Q^{1}_C$', ax=axs[0]);\n",
    "\n",
    "sns.lineplot(data=df_play_two, x='Episode', y='Q_D_cond_D', errorbar='sd', label=r'$Q^{2}_{D | D}$', ax=axs[1])\n",
    "sns.lineplot(data=df_play_two, x='Episode', y='Q_D_cond_C', errorbar='sd', label=r'$Q^{2}_{D | C}$', ax=axs[1])\n",
    "sns.lineplot(data=df_play_two, x='Episode', y='Q_C_cond_D', errorbar='sd', label=r'$Q^{2}_{C | D}$', ax=axs[1])\n",
    "sns.lineplot(data=df_play_two, x='Episode', y='Q_C_cond_C', errorbar='sd', label=r'$Q^{2}_{C | C}$', ax=axs[1]);\n",
    "\n",
    "axs[0].legend(bbox_to_anchor=(1,1))\n",
    "axs[1].legend(bbox_to_anchor=(1,1))\n",
    "axs[0].set_title('Agent 1 (does not have observations)')\n",
    "axs[1].set_title('Agent 2 (actions conditioned on agent 1s action)')\n",
    "\n",
    "axs[0].set_ylabel('Q-values player 1')\n",
    "axs[1].set_ylabel('Q-values player 2')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_play_one, x='Episode', y='Rewards', label='Play 1')\n",
    "sns.lineplot(data=df_play_two, x='Episode', y='Rewards', label='Play 2')\n",
    "plt.legend(bbox_to_anchor=(1,1));\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xraBLt1W0uOs",
    "Ik27P84pLDCS"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d34d5eef7507bb13b430f5217be103884edd667ef694ed18d3f7943da64c9dae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
