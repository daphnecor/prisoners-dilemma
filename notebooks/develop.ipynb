{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop notebook\n",
    "\n",
    "> **Todos**\n",
    "\n",
    "- [ ] Sanity check implementation of IPD where agent two sees the actions of agent one\n",
    "- [ ] Test functions and visualize Q-value trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from gym import Space, spaces\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from prisoners_dilemma import utils\n",
    "from prisoners_dilemma.env import PrisonersDilemmaEnv\n",
    "\n",
    "sns.set('notebook', font_scale=1.1, rc={'figure.figsize': (7, 4)})\n",
    "sns.set_style('ticks', rc={'figure.facecolor': 'none', 'axes.facecolor': 'none'})\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup config\n",
    "config = {\n",
    "    'num_simuls': 1,\n",
    "    'num_episodes': 500,\n",
    "    'num_agents': 2,\n",
    "    'num_actions': 2,\n",
    "    'verbose': False,\n",
    "    'init_type':'zeros',\n",
    "}\n",
    "\n",
    "config['payoffs'] = {\n",
    "    'reward_payoff': 2,\n",
    "    'tempta_payoff': 3,\n",
    "    'sucker_payoff': 0,\n",
    "    'punish_payoff': 1,\n",
    "}\n",
    "\n",
    "config['params'] = {\n",
    "    'alpha': np.array([0.1, 0.1]),\n",
    "    'eps': np.array([0.2, 0.2]),\n",
    "    'gamma': np.array([0.5, 0.5]),\n",
    "}\n",
    "\n",
    "\n",
    "#q_traj_one, q_traj_two, rewards_seq, actions_seq = run_extended_ipd_exp(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extended_ipd_exp(config: Dict) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    '''\n",
    "    Allow one agent to see the action of the other.\n",
    "    '''\n",
    "\n",
    "    game_env = PrisonersDilemmaEnv(\n",
    "        config['payoffs']['reward_payoff'], \n",
    "        config['payoffs']['tempta_payoff'], \n",
    "        config['payoffs']['sucker_payoff'], \n",
    "        config['payoffs']['punish_payoff'],\n",
    "    )\n",
    "\n",
    "    # Initialize Q-tables (own act x other play act)\n",
    "    q_table_one = np.zeros((game_env.action_space.n))\n",
    "    q_table_two = np.zeros((game_env.action_space.n, game_env.action_space.n))\n",
    "\n",
    "    q_traj_one = np.zeros((config['num_episodes'], game_env.action_space.n))\n",
    "    q_traj_two = np.zeros((config['num_episodes'], game_env.action_space.n, game_env.action_space.n))\n",
    "    rewards_seq = np.zeros((config['num_episodes'], config['num_agents']))\n",
    "    action_seq = np.zeros((config['num_episodes'], config['num_agents']), dtype=int)\n",
    "\n",
    "    # Condition the action of one agent on the action of the other\n",
    "    for episode_i in range(config['num_episodes']):\n",
    "\n",
    "        # # # # Select action player one # # # #\n",
    "        if np.random.random() < config['params']['eps'][0]:\n",
    "            act_play_one = np.array([game_env.action_space.sample()])\n",
    "        else: # Exploit\n",
    "            act_play_one = np.random.choice(\n",
    "                 a=np.argwhere((q_table_one == q_table_one.max())).flatten(),\n",
    "                size=(1,)\n",
    "            )\n",
    "        # # # # Select action player two | Condition on the action of player one # # # #\n",
    "        if np.random.random() < config['params']['eps'][1]:\n",
    "            act_play_two = np.array([game_env.action_space.sample()])\n",
    "        else:  # Exploit\n",
    "            act_play_two = np.random.choice(\n",
    "                a=np.argwhere((q_table_two[:, act_play_one] == q_table_two[:, act_play_one].max())).flatten(),\n",
    "                size=(1,)\n",
    "            )\n",
    "\n",
    "        # # # # Take a step # # # #\n",
    "        actions = np.concatenate([act_play_one, act_play_two])\n",
    "        _, rewards, _, _, _ = game_env.step(action=actions)\n",
    "\n",
    "        # # # # Update Q-values # # # #\n",
    "        q_table_one[act_play_one] = q_table_one[act_play_one] + \\\n",
    "            config['params']['alpha'][0] * (rewards[0] + config['params']['gamma'][0] * np.max(q_table_one) - q_table_one[act_play_one])\n",
    "\n",
    "        q_table_two[act_play_two, act_play_one] = q_table_two[act_play_two, act_play_one] + \\\n",
    "            config['params']['alpha'][1] * (rewards[1] + config['params']['gamma'][1] * np.max(q_table_two[:, act_play_one]) - q_table_two[act_play_two, act_play_one])\n",
    "\n",
    "        # Store trajectory\n",
    "        rewards_seq[episode_i, :] = rewards\n",
    "        action_seq[episode_i, :] = actions\n",
    "        # episode x actions x players\n",
    "        q_traj_one[episode_i, :] = q_table_one\n",
    "        q_traj_two[episode_i, :, :] = q_table_two\n",
    "\n",
    "    return (\n",
    "        q_traj_one,\n",
    "        q_traj_two,\n",
    "        rewards_seq,\n",
    "        action_seq,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d34d5eef7507bb13b430f5217be103884edd667ef694ed18d3f7943da64c9dae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
