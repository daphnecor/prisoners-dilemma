{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from gym import Space, spaces\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from prisoners_dilemma import utils\n",
    "from prisoners_dilemma.env import PrisonersDilemmaEnv\n",
    "\n",
    "sns.set('notebook', font_scale=1.1, rc={'figure.figsize': (7, 4)})\n",
    "sns.set_style('ticks', rc={'figure.facecolor': 'none', 'axes.facecolor': 'none'})\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup config\n",
    "config = {\n",
    "    'num_simuls': 10,\n",
    "    'num_episodes': 50,\n",
    "    'num_agents': 2,\n",
    "    'num_actions': 2,\n",
    "    'verbose': False,\n",
    "    'init_type':'zeros',\n",
    "}\n",
    "\n",
    "config['payoffs'] = {\n",
    "    'reward_payoff': 2,\n",
    "    'tempta_payoff': 3,\n",
    "    'sucker_payoff': 0,\n",
    "    'punish_payoff': 1,\n",
    "}\n",
    "\n",
    "config['params'] = {\n",
    "    'alpha': np.array([0.1, 0.1]),\n",
    "    'eps': np.array([0.1, 0.1]),\n",
    "    'gamma': np.array([0.5, 0.5]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QlearningAgent:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        env:object, \n",
    "        eps: float, \n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "        state: bool=None, \n",
    "        init_method: str=\"zeros\"\n",
    "        ):\n",
    "\n",
    "        self.env = env\n",
    "        self.eps = self._get_eps(eps)\n",
    "        self.alpha = self._get_step_size(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.state = state\n",
    "        self.q_table = self.init_q_table(init_method, table_size=env.action_space.n)\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def init_q_table(self, init_method: str, table_size: np.ndarray=2) -> np.ndarray:\n",
    "        \"\"\"Initialize Q-table.\n",
    "        \"\"\"\n",
    "        if init_method == \"zeros\":\n",
    "            return np.zeros((table_size))\n",
    "        raise NotImplementedError(\"Initialization method not known.\")\n",
    "\n",
    "    def get_action(self, state:object=None) -> int:\n",
    "        \"\"\"Agent takes a new action.\n",
    "\n",
    "        Args:\n",
    "            obs (_type_): _description_\n",
    "        \"\"\"\n",
    "        if np.random.random() < self._get_eps(self.eps):\n",
    "            action = np.array([self.env.action_space.sample()])\n",
    "        else: \n",
    "            action = np.random.choice(\n",
    "                a=np.argwhere((self.q_table == self.q_table.max())).flatten(),\n",
    "                size=(1,),\n",
    "            )\n",
    "        return action\n",
    "\n",
    "    def learn(self, action: int, reward: int, state: object=None) -> None:\n",
    "        \"\"\"Update Q-values for each state, action pair.\n",
    "\n",
    "        Args:\n",
    "            state (object): the state of the environment\n",
    "            action (int): action chosen by agent\n",
    "            reward (int): reward obtained in episode\n",
    "        \"\"\"\n",
    "\n",
    "        self.total_reward += reward\n",
    "\n",
    "        if state is not None:\n",
    "            pass #TODO: index q-table by the state and action\n",
    "        else:\n",
    "            self.q_table[action] = self.q_table[action] + self.alpha * (reward + self.gamma * np.max(self.q_table) - self.q_table[action])    \n",
    "    \n",
    "    def _get_eps(self, eps:float) -> float:\n",
    "        \"\"\"Get the exploration prob for an episode.\"\"\"\n",
    "        return eps\n",
    "\n",
    "    def _get_step_size(self, alpha:float) -> float:\n",
    "        \"\"\"Get the step size.\"\"\"\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "game_env = PrisonersDilemmaEnv(\n",
    "    config[\"payoffs\"][\"reward_payoff\"],\n",
    "    config[\"payoffs\"][\"tempta_payoff\"],\n",
    "    config[\"payoffs\"][\"sucker_payoff\"],\n",
    "    config[\"payoffs\"][\"punish_payoff\"],\n",
    ")\n",
    "\n",
    "# Create agents\n",
    "agent_one = QlearningAgent(\n",
    "    env=game_env,\n",
    "    eps=config['params']['eps'][0],\n",
    "    alpha=config['params']['alpha'][0],\n",
    "    gamma=config['params']['gamma'][0],\n",
    "    state=None,\n",
    ")\n",
    "\n",
    "agent_two = QlearningAgent(\n",
    "    env=game_env,\n",
    "    eps=config['params']['eps'][1],\n",
    "    alpha=config['params']['alpha'][1],\n",
    "    gamma=config['params']['gamma'][1],\n",
    "    state=None,\n",
    ")\n",
    "\n",
    "# Run for a number of episodes\n",
    "for episode_i in range(config['num_episodes']):\n",
    "    \n",
    "    # Get actions\n",
    "    act_one = agent_one.get_action()\n",
    "    act_two = agent_two.get_action()\n",
    "    actions = np.concatenate([act_one, act_two])\n",
    "\n",
    "    # Take a step\n",
    "    _, rewards, _, _, _ = game_env.step(action=actions)\n",
    "\n",
    "    # Learn: update Q-values\n",
    "    agent_one.learn(state=None, action=act_one, reward=rewards[0])\n",
    "    agent_two.learn(state=None, action=act_two, reward=rewards[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 51)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_one.total_reward, agent_two.total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.9659001 , 0.17175745]), array([1.96456682, 0.15487552]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_one.q_table.T, agent_two.q_table.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d34d5eef7507bb13b430f5217be103884edd667ef694ed18d3f7943da64c9dae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
